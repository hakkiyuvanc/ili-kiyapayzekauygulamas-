"""AI Service - LLM Entegrasyonu"""

import os
import json
from typing import Dict, Any, Optional, List
from openai import OpenAI
from anthropic import Anthropic


import google.generativeai as genai
from backend.app.core.config import settings

class AIService:
    """Yapay zeka servisi - OpenAI/Anthropic/Gemini entegrasyonu"""

    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        self.provider = settings.AI_PROVIDER
        
        # API anahtarlarÄ±nÄ± kontrol et
        if self.provider == "openai":
            api_key = settings.OPENAI_API_KEY
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
        elif self.provider == "anthropic":
            api_key = settings.ANTHROPIC_API_KEY
            if api_key:
                self.anthropic_client = Anthropic(api_key=api_key)
        elif self.provider == "gemini":
            api_key = settings.GEMINI_API_KEY
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_client = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        print(f"[{'SUCCESS' if self._is_available() else 'WARNING'}] AI Service initialized with provider: {self.provider}")

    def generate_insights(
        self,
        metrics: Dict[str, Any],
        conversation_summary: str,
        max_tokens: int = 1000
    ) -> List[Dict[str, str]]:
        """
        AI ile derinlemesine iÃ§gÃ¶rÃ¼ler oluÅŸtur
        
        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            Ä°Ã§gÃ¶rÃ¼ listesi
        """
        if not self._is_available():
            return self._fallback_insights(metrics)

        prompt = self._build_insights_prompt(metrics, conversation_summary)
        
        try:
            response = self._call_llm(prompt, max_tokens)
            insights = self._parse_insights_response(response)
            return insights
        except Exception as e:
            print(f"AI hatasÄ±: {e}")
            return self._fallback_insights(metrics)

    def generate_recommendations(
        self,
        metrics: Dict[str, Any],
        insights: List[Dict[str, str]],
        max_tokens: int = 800
    ) -> List[Dict[str, str]]:
        """
        AI ile kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler oluÅŸtur
        
        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            insights: OluÅŸturulan iÃ§gÃ¶rÃ¼ler
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            Ã–neri listesi
        """
        if not self._is_available():
            return self._fallback_recommendations(metrics)

        prompt = self._build_recommendations_prompt(metrics, insights)
        
        try:
            response = self._call_llm(prompt, max_tokens)
            recommendations = self._parse_recommendations_response(response)
            return recommendations
        except Exception as e:
            print(f"AI hatasÄ±: {e}")
            return self._fallback_recommendations(metrics)

    def chat_with_coach(
        self,
        message: str,
        history: List[Dict[str, str]],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        AI Ä°liÅŸki KoÃ§u ile sohbet et
        """
        if not self._is_available():
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda AI servislerine eriÅŸemiyorum. LÃ¼tfen daha sonra tekrar deneyin."

        system_prompt = """Sen profesyonel, empatik ve Ã§Ã¶zÃ¼m odaklÄ± bir Ä°liÅŸki KoÃ§usun. 
        KullanÄ±cÄ±larÄ±n iliÅŸki sorunlarÄ±nÄ± dinler, yargÄ±lamadan analiz eder ve yapÄ±cÄ± tavsiyeler verirsin.
        EÄŸer bir analiz raporu baÄŸlamÄ± varsa, cevaplarÄ±nÄ± bu rapora dayandÄ±r.
        KÄ±sa, net ve samimi cevaplar ver. Emoji kullanabilirsin."""

        if context:
            system_prompt += f"\n\nBAÄžLAM (Analiz Raporu):\n{json.dumps(context, ensure_ascii=False)}"

        messages = [{"role": "system", "content": system_prompt}]
        
        # History format: [{'role': 'user', 'content': '...'}, ...]
        # Limit history to last 10 messages to save tokens
        for msg in history[-10:]:
            messages.append({"role": msg["role"], "content": msg["content"]})
            
        messages.append({"role": "user", "content": message})

        try:
            if self.provider == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",  # or gpt-3.5-turbo
                    messages=messages,
                    max_tokens=500,
                    temperature=0.7
                )
                return response.choices[0].message.content
                
            elif self.provider == "gemini" and self.gemini_client:
                # Convert history to Gemini format
                gemini_history = []
                for msg in history[-10:]:
                    role = "user" if msg["role"] == "user" else "model"
                    gemini_history.append({"role": role, "parts": [msg["content"]]})
                
                # Add system prompt as the first message or configure it in model?
                # Gemini doesn't strictly have system message in chat history the same way.
                # We can prepend it to the first user message or use system_instruction if available in newer lib.
                # For compatibility, let's prepend system prompt context to the current message or start of history.
                
                # Simple approach: Prepend system prompt to the last message call for this turn
                full_message = f"{system_prompt}\n\nUSER MESSAGE: {message}"
                
                chat = self.gemini_client.start_chat(history=gemini_history)
                response = chat.send_message(full_message)
                return response.text
                
            # Default fallback
            return "AI saÄŸlayÄ±cÄ± yapÄ±landÄ±rmasÄ± eksik."
            
        except Exception as e:
            print(f"Chat error: {e}")
            return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."

    def enhance_summary(
        self,
        basic_summary: str,
        metrics: Dict[str, Any],
        max_tokens: int = 500
    ) -> str:
        """
        AI ile Ã¶zeti geliÅŸtir
        
        Args:
            basic_summary: Temel Ã¶zet
            metrics: Metrikler
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            GeliÅŸtirilmiÅŸ Ã¶zet
        """
        if not self._is_available():
            return basic_summary

        prompt = self._build_summary_prompt(basic_summary, metrics)
        
        try:
            return self._call_llm(prompt, max_tokens)
        except Exception:
            return basic_summary

    def _build_insights_prompt(self, metrics: Dict[str, Any], summary: str) -> str:
        """Ä°Ã§gÃ¶rÃ¼ promptu oluÅŸtur"""
        return f"""Sen bir iliÅŸki psikoloÄŸusun. AÅŸaÄŸÄ±daki konuÅŸma analiz metriklerine gÃ¶re derinlemesine iÃ§gÃ¶rÃ¼ler Ã¼ret.

METRIKLER:
{json.dumps(metrics, ensure_ascii=False, indent=2)}

KONUÅžMA Ã–ZETI:
{summary}

LÃ¼tfen 4-6 adet iÃ§gÃ¶rÃ¼ Ã¼ret. Her iÃ§gÃ¶rÃ¼ ÅŸu formatta olmalÄ±:
- category: "GÃ¼Ã§lÃ¼ YÃ¶n", "GeliÅŸim AlanÄ±", veya "Dikkat NoktasÄ±"
- title: KÄ±sa baÅŸlÄ±k (max 50 karakter)
- description: DetaylÄ± aÃ§Ä±klama (100-150 karakter)

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver:
[
  {{"category": "GÃ¼Ã§lÃ¼ YÃ¶n", "title": "...", "description": "..."}},
  {{"category": "GeliÅŸim AlanÄ±", "title": "...", "description": "..."}}
]"""

    def _build_recommendations_prompt(self, metrics: Dict[str, Any], insights: List[Dict]) -> str:
        """Ã–neri promptu oluÅŸtur"""
        insights_text = json.dumps(insights, ensure_ascii=False, indent=2)
        
        return f"""Sen bir iliÅŸki koÃ§usun. AÅŸaÄŸÄ±daki metriklere ve iÃ§gÃ¶rÃ¼lere gÃ¶re uygulanabilir Ã¶neriler Ã¼ret.

METRIKLER:
{json.dumps(metrics, ensure_ascii=False, indent=2)}

Ä°Ã‡GÃ–RÃœLER:
{insights_text}

LÃ¼tfen 4-5 adet somut, uygulanabilir Ã¶neri Ã¼ret. Her Ã¶neri ÅŸu formatta olmalÄ±:
- category: "Ä°letiÅŸim", "Empati", "Ã‡atÄ±ÅŸma YÃ¶netimi", veya "BaÄŸ GÃ¼Ã§lendirme"
- title: KÄ±sa baÅŸlÄ±k (max 50 karakter)
- description: DetaylÄ±, uygulanabilir Ã¶neri (100-150 karakter)

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver:
[
  {{"category": "Ä°letiÅŸim", "title": "...", "description": "..."}},
  {{"category": "Empati", "title": "...", "description": "..."}}
]"""

    def _build_summary_prompt(self, basic_summary: str, metrics: Dict[str, Any]) -> str:
        """Ã–zet geliÅŸtirme promptu"""
        return f"""AÅŸaÄŸÄ±daki iliÅŸki analizi Ã¶zetini daha anlaÅŸÄ±lÄ±r ve empatik hale getir:

MEVCUT Ã–ZET:
{basic_summary}

METRIKLER:
- Duygu Skoru: {metrics.get('sentiment', {}).get('score', 0)}
- Empati Skoru: {metrics.get('empathy', {}).get('score', 0)}
- Ã‡atÄ±ÅŸma Skoru: {metrics.get('conflict', {}).get('score', 0)}

KÄ±sa (2-3 cÃ¼mle), destekleyici ve yapÄ±cÄ± bir Ã¶zet oluÅŸtur. TÃ¼rkÃ§e yaz."""

    def _call_llm(self, prompt: str, max_tokens: int) -> str:
        """LLM Ã§aÄŸrÄ±sÄ± yap"""
        if self.provider == "openai" and self.openai_client:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir iliÅŸki terapistisin."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        elif self.provider == "anthropic" and self.anthropic_client:
            response = self.anthropic_client.messages.create(
                model=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
                max_tokens=max_tokens,
                temperature=0.7,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text.strip()
            
        elif self.provider == "gemini" and self.gemini_client:
            response = self.gemini_client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.7,
                )
            )
            return response.text.strip()
        
        raise Exception("AI provider yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")

    def _parse_insights_response(self, response: str) -> List[Dict[str, str]]:
        """AI yanÄ±tÄ±ndan iÃ§gÃ¶rÃ¼leri parse et"""
        try:
            # JSON array'i bul ve parse et
            start = response.find('[')
            end = response.rfind(']') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                insights = json.loads(json_str)
                return insights[:6]  # Maksimum 6 iÃ§gÃ¶rÃ¼
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")
        
        return []

    def _parse_recommendations_response(self, response: str) -> List[Dict[str, str]]:
        """AI yanÄ±tÄ±ndan Ã¶nerileri parse et"""
        try:
            start = response.find('[')
            end = response.rfind(']') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                recommendations = json.loads(json_str)
                return recommendations[:5]  # Maksimum 5 Ã¶neri
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")
        
        return []

    
    def generate_reply_suggestions(
        self,
        metrics: Dict[str, Any],
        conversation_summary: str,
        max_tokens: int = 500
    ) -> List[str]:
        """
        AI ile cevap Ã¶nerileri oluÅŸtur
        
        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            Cevap Ã¶nerileri listesi
        """
        if not self._is_available():
            return self._fallback_reply_suggestions()

        prompt = self._build_reply_suggestions_prompt(metrics, conversation_summary)
        
        try:
            response = self._call_llm(prompt, max_tokens)
            suggestions = self._parse_reply_suggestions_response(response)
            return suggestions
        except Exception as e:
            print(f"AI cevap Ã¶nerisi hatasÄ±: {e}")
            return self._fallback_reply_suggestions()

    def _fallback_reply_suggestions(self) -> List[str]:
        """AI kullanÄ±lamadÄ±ÄŸÄ±nda varsayÄ±lan cevap Ã¶nerileri"""
        return [
            "AnlÄ±yorum, bu konuya farklÄ± bir aÃ§Ä±dan bakabiliriz.",
            "DuygularÄ±nÄ± paylaÅŸtÄ±ÄŸÄ±n iÃ§in teÅŸekkÃ¼r ederim, seni daha iyi anlamak istiyorum.",
            "Bu durum beni de dÃ¼ÅŸÃ¼ndÃ¼rÃ¼yor, ortak bir Ã§Ã¶zÃ¼m bulalÄ±m."
        ]

    def _build_reply_suggestions_prompt(self, metrics: Dict[str, Any], summary: str) -> str:
        """Cevap Ã¶nerileri iÃ§in prompt oluÅŸtur"""
        return f"""
        AÅŸaÄŸÄ±daki iliÅŸki analizine dayanarak, kullanÄ±cÄ±nÄ±n karÅŸÄ± tarafa yazabileceÄŸi 3 farklÄ± cevap seÃ§eneÄŸi Ã¶ner.
        
        Analiz Ã–zeti: {summary}
        Duygu Durumu: {metrics.get('sentiment', {}).get('score', 50)}/100
        Empati Seviyesi: {metrics.get('empathy', {}).get('score', 50)}/100
        
        LÃ¼tfen ÅŸu 3 farklÄ± tonda cevap Ã¶nerisi sun:
        1. YapÄ±cÄ± ve Ã‡Ã¶zÃ¼m OdaklÄ±
        2. Empatik ve Duygusal
        3. Net ve SÄ±nÄ±r Koyucu
        
        Format: Sadece 3 madde halinde cevap metinlerini yaz. BaÅŸka aÃ§Ä±klama ekleme.
        """

    def _parse_reply_suggestions_response(self, response: str) -> List[str]:
        """AI yanÄ±tÄ±ndan cevap Ã¶nerilerini ayÄ±kla"""
        try:
            # SatÄ±r satÄ±r ayÄ±r ve temizle
            lines = [line.strip() for line in response.strip().split('\n') if line.strip()]
            suggestions = []
            for line in lines:
                # NumaralandÄ±rmayÄ± temizle (1. , - vb.)
                cleaned = line.lstrip('1234567890.-*â€¢ ')
                if cleaned:
                    suggestions.append(cleaned)
            
            return suggestions[:3]  # En fazla 3 Ã¶neri
        except Exception:
            return self._fallback_reply_suggestions()

    def _is_available(self) -> bool:
        """AI servisi kullanÄ±labilir mi?"""
        return (self.openai_client is not None) or (self.anthropic_client is not None) or (self.gemini_client is not None)

    def _fallback_insights(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """AI yoksa fallback iÃ§gÃ¶rÃ¼ler"""
        insights = []
        
        # Duygu analizi
        sentiment_score = metrics.get("sentiment", {}).get("score", 0)
        if sentiment_score >= 60:
            insights.append({
                "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                "title": "Olumlu Ä°letiÅŸim",
                "description": "Ä°letiÅŸiminiz genel olarak pozitif ve destekleyici bir ton taÅŸÄ±yor.",
                "icon": "âœ…"
            })
        elif sentiment_score < 40:
            insights.append({
                "category": "GeliÅŸim AlanÄ±",
                "title": "Duygusal Ton",
                "description": "Ä°letiÅŸimde daha olumlu bir dil kullanmaya Ã¶zen gÃ¶sterin.",
                "icon": "ðŸ’¡"
            })
        
        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score >= 60:
            insights.append({
                "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                "title": "YÃ¼ksek Empati",
                "description": "KarÅŸÄ±nÄ±zdakinin duygularÄ±nÄ± anlamaya Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±z aÃ§Ä±kÃ§a gÃ¶rÃ¼lÃ¼yor.",
                "icon": "ðŸ’"
            })
        else:
            insights.append({
                "category": "GeliÅŸim AlanÄ±",
                "title": "Empati GeliÅŸtirme",
                "description": "Daha fazla empatik ifade kullanarak iletiÅŸimi gÃ¼Ã§lendirebilirsiniz.",
                "icon": "ðŸ’¡"
            })
        
        # Ã‡atÄ±ÅŸma
        conflict_score = metrics.get("conflict", {}).get("score", 0)
        if conflict_score > 50:
            insights.append({
                "category": "Dikkat NoktasÄ±",
                "title": "Ã‡atÄ±ÅŸma Belirtileri",
                "description": "KonuÅŸmada gerginlik iÅŸaretleri var. Sakin ve yapÄ±cÄ± iletiÅŸime odaklanÄ±n.",
                "icon": "âš ï¸"
            })
        
        return insights

    def _fallback_recommendations(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """AI yoksa fallback Ã¶neriler"""
        recommendations = []
        
        # Biz-dili
        we_score = metrics.get("we_language", {}).get("score", 0)
        if we_score < 40:
            recommendations.append({
                "category": "BaÄŸ GÃ¼Ã§lendirme",
                "title": "Biz-dili KullanÄ±n",
                "description": "'Biz', 'bizim' gibi kelimeler kullanarak ortak hedeflerinizi vurgulayÄ±n."
            })
        
        # Denge
        balance_score = metrics.get("communication_balance", {}).get("score", 0)
        if balance_score < 50:
            recommendations.append({
                "category": "Ä°letiÅŸim",
                "title": "Ä°letiÅŸim Dengesi",
                "description": "Her iki taraf da eÅŸit ÅŸekilde kendini ifade etmeye Ã§alÄ±ÅŸsÄ±n."
            })
        
        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score < 60:
            recommendations.append({
                "category": "Empati",
                "title": "Aktif Dinleme",
                "description": "KarÅŸÄ±nÄ±zÄ± dinlerken 'AnladÄ±m', 'Seni anlÄ±yorum' gibi ifadeler kullanÄ±n."
            })
        
        return recommendations


# Singleton instance
_ai_service_instance = None


def get_ai_service() -> AIService:
    """AI service singleton"""
    global _ai_service_instance
    if _ai_service_instance is None:
        _ai_service_instance = AIService()
    return _ai_service_instance
