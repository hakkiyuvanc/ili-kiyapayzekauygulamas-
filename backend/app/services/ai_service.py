"""AI Service - LLM Entegrasyonu"""

import hashlib
import json
import logging
import os
import time
from datetime import datetime
from typing import Any, Optional

import google.generativeai as genai
from anthropic import Anthropic
from openai import OpenAI
from pydantic import ValidationError

from app.core.config import settings
from app.schemas.ai_responses import (
    Insight,
    InsightsResponse,
    Recommendation,
    RecommendationsResponse,
    RelationshipReport,
)
from app.services.cache_service import cache_service
from app.services.knowledge_base import format_knowledge_context, get_relevant_knowledge

logger = logging.getLogger(__name__)


class AIService:
    """Yapay zeka servisi - OpenAI/Anthropic/Gemini entegrasyonu"""

    PROMPT_VERSION = "v3.0"  # Prompt versioning - Strict JSON with Pydantic

    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        self.provider = settings.AI_PROVIDER

        # API anahtarlarÄ±nÄ± kontrol et
        if self.provider == "openai":
            api_key = settings.OPENAI_API_KEY
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
        elif self.provider == "anthropic":
            api_key = settings.ANTHROPIC_API_KEY
            if api_key:
                self.anthropic_client = Anthropic(api_key=api_key)
        elif self.provider == "gemini":
            api_key = settings.GEMINI_API_KEY
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_client = genai

        # Structured logging
        if self._is_available():
            logger.info(
                "AI Service initialized",
                extra={
                    "provider": self.provider,
                    "prompt_version": self.PROMPT_VERSION,
                    "status": "available",
                },
            )
        else:
            logger.warning(
                "AI Service initialized without provider",
                extra={"provider": self.provider, "status": "fallback_mode"},
            )

    def generate_insights(
        self, metrics: dict[str, Any], conversation_summary: str, max_tokens: int = 1200
    ) -> list[dict[str, str]]:
        """
        AI ile derinlemesine iÃ§gÃ¶rÃ¼ler oluÅŸtur (V3.0 - Strict JSON with Pydantic)

        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±

        Returns:
            Ä°Ã§gÃ¶rÃ¼ listesi (dict format for backward compatibility)
        """
        start_time = time.time()

        # Check cache first
        cache_key = self._get_cache_key("insights_v3", metrics, conversation_summary)
        cached = cache_service.get(cache_key)

        if cached:
            logger.info(
                "AI insights cache hit",
                extra={
                    "cache_key": cache_key[:20],
                    "latency_ms": (time.time() - start_time) * 1000,
                },
            )
            return cached

        if not self._is_available():
            fallback = self._fallback_insights(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

        try:
            # Build prompt for structured output
            prompt = self._build_insights_prompt_v3(metrics, conversation_summary)

            # Call with Pydantic validation
            validated_response = self._call_llm_structured(
                prompt=prompt, response_model=InsightsResponse, max_tokens=max_tokens
            )

            # Convert Pydantic models to dict for backward compatibility
            insights = [insight.model_dump() for insight in validated_response.insights]

            # Cache successful response
            cache_service.set(cache_key, insights, ttl_seconds=3600)

            # Log success
            logger.info(
                "AI insights generated successfully (V3.0)",
                extra={
                    "provider": self.provider,
                    "insights_count": len(insights),
                    "latency_ms": (time.time() - start_time) * 1000,
                    "prompt_version": self.PROMPT_VERSION,
                },
            )

            return insights

        except Exception as e:
            logger.error(
                "AI insights generation failed",
                extra={
                    "error": str(e),
                    "provider": self.provider,
                    "latency_ms": (time.time() - start_time) * 1000,
                },
                exc_info=True,
            )

            fallback = self._fallback_insights(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

    def generate_recommendations(
        self, metrics: dict[str, Any], insights: list[dict[str, str]], max_tokens: int = 1000
    ) -> list[dict[str, str]]:
        """
        AI ile kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler oluÅŸtur (V3.0 - Strict JSON with Pydantic)
        """
        start_time = time.time()

        # Check cache
        cache_key = self._get_cache_key("recommendations_v3", metrics, str(insights))
        cached = cache_service.get(cache_key)

        if cached:
            logger.info(
                "AI recommendations cache hit",
                extra={"latency_ms": (time.time() - start_time) * 1000},
            )
            return cached

        if not self._is_available():
            fallback = self._fallback_recommendations(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

        try:
            # Build prompt for structured output
            prompt = self._build_recommendations_prompt_v3(metrics, insights)

            # Call with Pydantic validation
            validated_response = self._call_llm_structured(
                prompt=prompt, response_model=RecommendationsResponse, max_tokens=max_tokens
            )

            # Convert Pydantic models to dict for backward compatibility
            recommendations = [rec.model_dump() for rec in validated_response.recommendations]

            # Cache and log
            cache_service.set(cache_key, recommendations, ttl_seconds=3600)

            logger.info(
                "AI recommendations generated (V3.0)",
                extra={
                    "provider": self.provider,
                    "recommendations_count": len(recommendations),
                    "latency_ms": (time.time() - start_time) * 1000,
                    "prompt_version": self.PROMPT_VERSION,
                },
            )

            return recommendations

        except Exception as e:
            logger.error(
                "AI recommendations generation failed",
                extra={
                    "error": str(e),
                    "provider": self.provider,
                    "latency_ms": (time.time() - start_time) * 1000,
                },
                exc_info=True,
            )

            fallback = self._fallback_recommendations(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

    def chat_with_coach(
        self, message: str, history: list[dict[str, str]], context: Optional[dict[str, Any]] = None
    ) -> str:
        """
        AI Ä°liÅŸki KoÃ§u ile sohbet et
        """
        if not self._is_available():
            return (
                "ÃœzgÃ¼nÃ¼m, ÅŸu anda AI servislerine eriÅŸemiyorum. LÃ¼tfen daha sonra tekrar deneyin."
            )

        system_prompt = """Sen profesyonel, empatik ve Ã§Ã¶zÃ¼m odaklÄ± bir Ä°liÅŸki KoÃ§usun.
        KullanÄ±cÄ±larÄ±n iliÅŸki sorunlarÄ±nÄ± dinler, yargÄ±lamadan analiz eder ve yapÄ±cÄ± tavsiyeler verirsin.
        EÄŸer bir analiz raporu baÄŸlamÄ± varsa, cevaplarÄ±nÄ± bu rapora dayandÄ±r.
        KÄ±sa, net ve samimi cevaplar ver. Emoji kullanabilirsin."""

        if context:
            system_prompt += (
                f"\n\nBAÄžLAM (Analiz Raporu):\n{json.dumps(context, ensure_ascii=False)}"
            )

        messages = [{"role": "system", "content": system_prompt}]

        # History format: [{'role': 'user', 'content': '...'}, ...]
        # Limit history to last 10 messages to save tokens
        for msg in history[-10:]:
            messages.append({"role": msg["role"], "content": msg["content"]})

        messages.append({"role": "user", "content": message})

        try:
            if self.provider == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=messages,
                    max_tokens=500,
                    temperature=0.7,
                )
                return response.choices[0].message.content

            elif self.provider == "gemini" and self.gemini_client:
                # Build chat history for Gemini
                chat_history = []

                # Add history (excluding system prompt)
                for msg in history[-10:]:
                    role = "user" if msg["role"] == "user" else "model"
                    chat_history.append({"role": role, "parts": [msg["content"]]})

                # Create chat session
                model = genai.GenerativeModel(
                    model_name=settings.GEMINI_MODEL, system_instruction=system_prompt
                )
                chat = model.start_chat(history=chat_history)

                # Send message
                response = chat.send_message(
                    message,
                    generation_config=genai.GenerationConfig(
                        max_output_tokens=500,
                        temperature=0.7,
                    ),
                )
                return response.text

            # Default fallback
            return "AI saÄŸlayÄ±cÄ± yapÄ±landÄ±rmasÄ± eksik."

        except Exception as e:
            logger.error(
                "AI chat failed",
                extra={"error": str(e), "provider": self.provider, "message_length": len(message)},
                exc_info=True,
            )
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda bir sorun yaÅŸÄ±yorum. LÃ¼tfen daha sonra tekrar deneyin."

    def enhance_summary(
        self, basic_summary: str, metrics: dict[str, Any], max_tokens: int = 500
    ) -> str:
        """
        AI ile Ã¶zeti geliÅŸtir

        Args:
            basic_summary: Temel Ã¶zet
            metrics: Metrikler
            max_tokens: Maksimum token sayÄ±sÄ±

        Returns:
            GeliÅŸtirilmiÅŸ Ã¶zet
        """
        if not self._is_available():
            return basic_summary

        prompt = self._build_summary_prompt(basic_summary, metrics)

        try:
            return self._call_llm(prompt, max_tokens)
        except Exception:
            return basic_summary

    def _build_insights_prompt(self, metrics: dict[str, Any], summary: str) -> str:
        """Ä°Ã§gÃ¶rÃ¼ promptu oluÅŸtur (improved with few-shot & chain-of-thought & knowledge)"""

        # Context optimization: Extract top metrics only
        top_metrics = {
            "sentiment": metrics.get("sentiment", {}),
            "empathy": metrics.get("empathy", {}),
            "conflict": metrics.get("conflict", {}),
            "we_language": metrics.get("we_language", {}),
        }

        # RAG Quick Win: Get relevant knowledge snippets
        knowledge = get_relevant_knowledge(metrics)
        knowledge_context = format_knowledge_context(knowledge)

        return f"""
{knowledge_context}
Sen bir iliÅŸki psikoloÄŸusun. AÅŸaÄŸÄ±daki gÃ¶revi adÄ±m adÄ±m yap:

1. ADIM: Metrikleri incele
{json.dumps(top_metrics, indent=2, ensure_ascii=False)}

2. ADIM: KonuÅŸma Ã¶zetini oku
{summary}

3. ADIM: YukarÄ±daki psikoloji bilgilerini referans alarak metriklere gÃ¶re iÃ§gÃ¶rÃ¼ler Ã¼ret

Ã–RNEK Ã‡IKTILAR (Referans iÃ§in):
[
  {{
    "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
    "title": "YÃ¼ksek Empati Seviyesi",
    "description": "Ä°letiÅŸimde karÅŸÄ±nÄ±zÄ± anlamaya yÃ¶nelik gÃ¼Ã§lÃ¼ Ã§aba var. Bu, iliÅŸkide gÃ¼ven ve yakÄ±nlÄ±k oluÅŸturmanÄ±n temel taÅŸÄ±dÄ±r."
  }},
  {{
    "category": "GeliÅŸim AlanÄ±",
    "title": "Biz-dili KullanÄ±mÄ± ZayÄ±f",
    "description": "Bireysel ifadeler aÄŸÄ±rlÄ±kta. 'Biz', 'birlikte' gibi kelimeler kullanarak ortaklÄ±k hissini gÃ¼Ã§lendirebilirsiniz."
  }},
  {{
    "category": "Dikkat NoktasÄ±",
    "title": "Ã‡atÄ±ÅŸma YÃ¶netimi",
    "description": "AnlaÅŸmazlÄ±klarda savunma moduna geÃ§me eÄŸilimi var. AÃ§Ä±k ve sakin iletiÅŸim Ã¶nemli."
  }}
]

4. ADIM: YukarÄ±daki metriklere ve psikoloji bilgilerine gÃ¶re 4-6 adet benzeri iÃ§gÃ¶rÃ¼ Ã¼ret

FORMAT KURALLARI:
- category: sadece "GÃ¼Ã§lÃ¼ YÃ¶n", "GeliÅŸim AlanÄ±", veya "Dikkat NoktasÄ±"
- title: max 50 karakter, TÃ¼rkÃ§e
- description: 100-150 karakter, empatik ve destekleyici ton

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver (Markdown yok):
[
  {"category": "GÃ¼Ã§lÃ¼ YÃ¶n", "title": "...", "description": "..."},
  {"category": "GeliÅŸim AlanÄ±", "title": "...", "description": "..."}
]"""

    def _build_recommendations_prompt(self, metrics: dict[str, Any], insights: list[dict]) -> str:
        """Ã–neri promptu oluÅŸtur (improved with few-shot)"""

        # Context optimization: Top insights only
        top_insights = insights[:4] if len(insights) > 4 else insights

        return f"""
Sen bir iliÅŸki koÃ§usun. AÅŸaÄŸÄ±daki gÃ¶revi adÄ±m adÄ±m yap:

1. ADIM: Ä°Ã§gÃ¶rÃ¼leri oku
{json.dumps(top_insights, indent=2, ensure_ascii=False)}

2. ADIM: Metriklerdeki zayÄ±f alanlarÄ± tespit et
- Empati skoru: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma skoru: {metrics.get('conflict', {}).get('score', 'N/A')}
- Biz-dili skoru: {metrics.get('we_language', {}).get('score', 'N/A')}

3. ADIM: Somut, uygulanabilir Ã¶neriler oluÅŸtur

Ã–RNEK Ã‡IKTILAR (Referans iÃ§in):
[
  {{
    "category": "BaÄŸ GÃ¼Ã§lendirme",
    "title": "Ortak Hedefler Belirleyin",
    "description": "'Bizim iÃ§in ne iyi?' sorusunu sorun. HaftalÄ±k bir 'biz' planÄ± yapÄ±n ve birlikte karar alÄ±n."
  }},
  {{
    "category": "Ä°letiÅŸim",
    "title": "GÃ¼nlÃ¼k Check-in Rutini",
    "description": "Her gÃ¼n 10 dakika telefonlar kapalÄ± konuÅŸun. Sadece dinleyin ve 'AnlÄ±yorum' deyin."
  }},
  {{
    "category": "Empati",
    "title": "Duygu YansÄ±tma PratiÄŸi",
    "description": "KarÅŸÄ±nÄ±zÄ±n sÃ¶ylediklerini kendi cÃ¼mlelerinizle tekrarlayÄ±n. 'Senin iÃ§in bu zor olmalÄ±' gibi."
  }}
]

4. ADIM: YukarÄ±daki iÃ§gÃ¶rÃ¼lere gÃ¶re 4-5 adet benzeri Ã¶neri Ã¼ret

FORMAT KURALLARI:
- category: "Ä°letiÅŸim", "Empati", "Ã‡atÄ±ÅŸma YÃ¶netimi", veya "BaÄŸ GÃ¼Ã§lendirme"
- title: max 50 karakter, eyleme yÃ¶nelik
- description: 100-150 karakter, somut adÄ±mlar iÃ§ermeli

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver:
"""

    def _build_summary_prompt(self, basic_summary: str, metrics: dict[str, Any]) -> str:
        """Ã–zet geliÅŸtirme promptu"""
        return f"""
AÅŸaÄŸÄ±daki iliÅŸki analizi Ã¶zetini daha anlaÅŸÄ±lÄ±r ve empatik hale getir:

MEVCUT Ã–ZET:
{basic_summary}

METRIKLER:
- Duygu Skoru: {metrics.get('sentiment', {}).get('score', 'N/A')}
- Empati Skoru: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma Skoru: {metrics.get('conflict', {}).get('score', 'N/A')}

KÄ±sa (2-3 cÃ¼mle), destekleyici ve yapÄ±cÄ± bir Ã¶zet oluÅŸtur. TÃ¼rkÃ§e kullan.
"""

    def _build_simple_insights_prompt(self, metrics: dict[str, Any]) -> str:
        """Simplified prompt for retry (error recovery)"""
        return f"""
Ä°letiÅŸim analizi iÃ§in iÃ§gÃ¶rÃ¼ler Ã¼ret:

Metrikler:
- Duygu: {metrics.get('sentiment', {}).get('score', 'N/A')}
- Empati: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma: {metrics.get('conflict', {}).get('score', 'N/A')}

3-4 kÄ±sa iÃ§gÃ¶rÃ¼ JSON formatÄ±nda ver:
[{{"category": "GÃ¼Ã§lÃ¼ YÃ¶n", "title": "...", "description": "..."}}]
"""

    def _call_llm(self, prompt: str, max_tokens: int) -> str:
        """LLM Ã§aÄŸrÄ±sÄ± yap"""
        if self.provider == "openai" and self.openai_client:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {
                        "role": "system",
                        "content": "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir iliÅŸki terapistisin.",
                    },
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                temperature=0.7,
            )
            return response.choices[0].message.content.strip()

        elif self.provider == "anthropic" and self.anthropic_client:
            response = self.anthropic_client.messages.create(
                model=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
                max_tokens=max_tokens,
                temperature=0.7,
                messages=[{"role": "user", "content": prompt}],
            )
            return response.content[0].text.strip()

        elif self.provider == "gemini" and self.gemini_client:
            model = genai.GenerativeModel(model_name=settings.GEMINI_MODEL)
            response = model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.7,
                ),
            )
            return response.text.strip()

        raise Exception("AI provider yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")

    def _call_llm_structured(
        self, prompt: str, response_model: type, max_tokens: int, max_retries: int = 2
    ):
        """
        LLM Ã§aÄŸrÄ±sÄ± yap ve Pydantic modeli ile validate et (V3.0)

        Args:
            prompt: System + user prompt
            response_model: Pydantic model class (e.g., InsightsResponse)
            max_tokens: Max token count
            max_retries: Retry count on validation failure

        Returns:
            Validated Pydantic model instance

        Raises:
            ValidationError: If JSON doesn't match schema after retries
        """
        for attempt in range(max_retries + 1):
            try:
                # Get JSON schema from Pydantic model
                schema = response_model.model_json_schema()

                # Enhanced prompt with schema
                structured_prompt = f"""{prompt}

CRITICAL: Your response MUST be valid JSON matching this exact schema:
{json.dumps(schema, indent=2, ensure_ascii=False)}

Rules:
- Return ONLY the JSON object, no markdown, no explanations
- All required fields must be present
- Follow min/max constraints exactly
- Use Turkish language for text fields"""

                # Call LLM
                if self.provider == "openai" and self.openai_client:
                    # Try to use JSON mode if available
                    try:
                        response = self.openai_client.chat.completions.create(
                            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                            messages=[
                                {
                                    "role": "system",
                                    "content": "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir iliÅŸki terapistisin. Sadece geÃ§erli JSON dÃ¶ndÃ¼r.",
                                },
                                {"role": "user", "content": structured_prompt},
                            ],
                            max_tokens=max_tokens,
                            temperature=0.7,
                            response_format={"type": "json_object"},  # JSON mode
                        )
                        raw_response = response.choices[0].message.content.strip()
                    except Exception:
                        # Fallback without JSON mode
                        response = self.openai_client.chat.completions.create(
                            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                            messages=[
                                {
                                    "role": "system",
                                    "content": "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir iliÅŸki terapistisin.",
                                },
                                {"role": "user", "content": structured_prompt},
                            ],
                            max_tokens=max_tokens,
                            temperature=0.7,
                        )
                        raw_response = response.choices[0].message.content.strip()

                elif self.provider == "anthropic" and self.anthropic_client:
                    response = self.anthropic_client.messages.create(
                        model=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
                        max_tokens=max_tokens,
                        temperature=0.7,
                        messages=[{"role": "user", "content": structured_prompt}],
                    )
                    raw_response = response.content[0].text.strip()

                elif self.provider == "gemini" and self.gemini_client:
                    model = genai.GenerativeModel(
                        model_name=settings.GEMINI_MODEL,
                        generation_config=genai.GenerationConfig(
                            response_mime_type="application/json",
                            max_output_tokens=max_tokens,
                            temperature=0.7,
                        ),
                    )
                    response = model.generate_content(structured_prompt)
                    raw_response = response.text.strip()
                else:
                    raise Exception("AI provider yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")

                # Extract JSON if wrapped in markdown
                if "```json" in raw_response:
                    start = raw_response.find("```json") + 7
                    end = raw_response.rfind("```")
                    raw_response = raw_response[start:end].strip()
                elif "```" in raw_response:
                    start = raw_response.find("```") + 3
                    end = raw_response.rfind("```")
                    raw_response = raw_response[start:end].strip()

                # Parse and validate with Pydantic
                parsed_data = json.loads(raw_response)
                validated = response_model.model_validate(parsed_data)

                logger.info(
                    "Structured LLM call successful",
                    extra={
                        "model": response_model.__name__,
                        "attempt": attempt + 1,
                        "provider": self.provider,
                    },
                )

                return validated

            except (json.JSONDecodeError, ValidationError) as e:
                logger.warning(
                    f"Structured LLM validation failed (attempt {attempt + 1}/{max_retries + 1})",
                    extra={"error": str(e), "model": response_model.__name__},
                )

                if attempt == max_retries:
                    # Final attempt failed
                    logger.error(
                        "Structured LLM call failed after all retries",
                        extra={
                            "model": response_model.__name__,
                            "raw_response": raw_response[:200],
                        },
                    )
                    raise

                # Add error feedback to next attempt
                prompt = f"""{prompt}

PREVIOUS ATTEMPT FAILED with error: {str(e)}
Please fix the JSON structure and try again."""

        raise Exception("Structured LLM call failed")

    def _parse_insights_response(self, response: str) -> list[dict[str, str]]:
        """AI yanÄ±tÄ±ndan iÃ§gÃ¶rÃ¼leri parse et"""
        try:
            # JSON array'i bul ve parse et
            start = response.find("[")
            end = response.rfind("]") + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                insights = json.loads(json_str)
                return insights[:6]  # Maksimum 6 iÃ§gÃ¶rÃ¼
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")

        return []

    def _parse_recommendations_response(self, response: str) -> list[dict[str, str]]:
        """AI yanÄ±tÄ±ndan Ã¶nerileri parse et"""
        try:
            start = response.find("[")
            end = response.rfind("]") + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                recommendations = json.loads(json_str)
                return recommendations[:5]  # Maksimum 5 Ã¶neri
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")

        return []

    def generate_reply_suggestions(
        self, metrics: dict[str, Any], conversation_summary: str, max_tokens: int = 500
    ) -> list[str]:
        """
        AI ile cevap Ã¶nerileri oluÅŸtur

        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±

        Returns:
            Cevap Ã¶nerileri listesi
        """
        if not self._is_available():
            return self._fallback_reply_suggestions()

        prompt = self._build_reply_suggestions_prompt(metrics, conversation_summary)

        try:
            response = self._call_llm(prompt, max_tokens)
            suggestions = self._parse_reply_suggestions_response(response)
            return suggestions
        except Exception as e:
            logger.error(
                "AI reply suggestions failed",
                extra={"error": str(e), "provider": self.provider},
                exc_info=True,
            )
            return self._fallback_reply_suggestions()

    def _fallback_reply_suggestions(self) -> list[str]:
        """AI kullanÄ±lamadÄ±ÄŸÄ±nda varsayÄ±lan cevap Ã¶nerileri"""
        return [
            "AnlÄ±yorum, bu konuya farklÄ± bir aÃ§Ä±dan bakabiliriz.",
            "DuygularÄ±nÄ± paylaÅŸtÄ±ÄŸÄ±n iÃ§in teÅŸekkÃ¼r ederim, seni daha iyi anlamak istiyorum.",
            "Bu durum beni de dÃ¼ÅŸÃ¼ndÃ¼rÃ¼yor, ortak bir Ã§Ã¶zÃ¼m bulalÄ±m.",
        ]

    def _build_reply_suggestions_prompt(self, metrics: dict[str, Any], summary: str) -> str:
        """Cevap Ã¶nerileri iÃ§in prompt oluÅŸtur"""
        return f"""
        AÅŸaÄŸÄ±daki iliÅŸki analizine dayanarak, kullanÄ±cÄ±nÄ±n karÅŸÄ± tarafa yazabileceÄŸi 3 farklÄ± cevap seÃ§eneÄŸi Ã¶ner.

        Analiz Ã–zeti: {summary}
        Duygu Durumu: {metrics.get('sentiment', {}).get('score', 50)}/100
        Empati Seviyesi: {metrics.get('empathy', {}).get('score', 50)}/100

        LÃ¼tfen ÅŸu 3 farklÄ± tonda cevap Ã¶nerisi sun:
        1. YapÄ±cÄ± ve Ã‡Ã¶zÃ¼m OdaklÄ±
        2. Empatik ve Duygusal
        3. Net ve SÄ±nÄ±r Koyucu

        Format: Sadece 3 madde halinde cevap metinlerini yaz. BaÅŸka aÃ§Ä±klama ekleme.
        """

    def _parse_reply_suggestions_response(self, response: str) -> list[str]:
        """AI yanÄ±tÄ±ndan cevap Ã¶nerilerini ayÄ±kla"""
        try:
            # SatÄ±r satÄ±r ayÄ±r ve temizle
            lines = [line.strip() for line in response.strip().split("\n") if line.strip()]
            suggestions = []
            for line in lines:
                # NumaralandÄ±rmayÄ± temizle (1. , - vb.)
                cleaned = line.lstrip("1234567890.-*â€¢ ")
                if cleaned:
                    suggestions.append(cleaned)

            return suggestions[:3]  # En fazla 3 Ã¶neri
        except Exception:
            return self._fallback_reply_suggestions()

    def _get_cache_key(self, operation: str, metrics: dict, context: str = "") -> str:
        """Generate cache key for AI responses"""
        data = {
            "operation": operation,
            "metrics": {
                k: v.get("score") if isinstance(v, dict) else v for k, v in metrics.items()
            },
            "context": context[:100] if context else "",  # First 100 chars only
            "version": self.PROMPT_VERSION,
        }
        data_str = json.dumps(data, sort_keys=True)
        hash_key = hashlib.md5(data_str.encode()).hexdigest()
        return f"ai_{operation}:{hash_key}"

    def _is_available(self) -> bool:
        """AI servisi kullanÄ±labilir mi?"""
        return (
            (self.openai_client is not None)
            or (self.anthropic_client is not None)
            or (self.gemini_client is not None)
        )

    def _fallback_insights(self, metrics: dict[str, Any]) -> list[dict[str, str]]:
        """AI yoksa fallback iÃ§gÃ¶rÃ¼ler"""
        insights = []

        # Duygu analizi
        sentiment_score = metrics.get("sentiment", {}).get("score", 0)
        if sentiment_score >= 60:
            insights.append(
                {
                    "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                    "title": "Olumlu Ä°letiÅŸim",
                    "description": "Ä°letiÅŸiminiz genel olarak pozitif ve destekleyici bir ton taÅŸÄ±yor.",
                    "icon": "âœ…",
                }
            )
        elif sentiment_score < 40:
            insights.append(
                {
                    "category": "GeliÅŸim AlanÄ±",
                    "title": "Duygusal Ton",
                    "description": "Ä°letiÅŸimde daha olumlu bir dil kullanmaya Ã¶zen gÃ¶sterin.",
                    "icon": "ðŸ’¡",
                }
            )

        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score >= 60:
            insights.append(
                {
                    "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                    "title": "YÃ¼ksek Empati",
                    "description": "KarÅŸÄ±nÄ±zdakinin duygularÄ±nÄ± anlamaya Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±z aÃ§Ä±kÃ§a gÃ¶rÃ¼lÃ¼yor.",
                    "icon": "ðŸ’",
                }
            )
        else:
            insights.append(
                {
                    "category": "GeliÅŸim AlanÄ±",
                    "title": "Empati GeliÅŸtirme",
                    "description": "Daha fazla empatik ifade kullanarak iletiÅŸimi gÃ¼Ã§lendirebilirsiniz.",
                    "icon": "ðŸ’¡",
                }
            )

        # Ã‡atÄ±ÅŸma
        conflict_score = metrics.get("conflict", {}).get("score", 0)
        if conflict_score > 50:
            insights.append(
                {
                    "category": "Dikkat NoktasÄ±",
                    "title": "Ã‡atÄ±ÅŸma Belirtileri",
                    "description": "KonuÅŸmada gerginlik iÅŸaretleri var. Sakin ve yapÄ±cÄ± iletiÅŸime odaklanÄ±n.",
                    "icon": "âš ï¸",
                }
            )

        return insights

    def _fallback_recommendations(self, metrics: dict[str, Any]) -> list[dict[str, str]]:
        """AI yoksa fallback Ã¶neriler"""
        recommendations = []

        # Biz-dili
        we_score = metrics.get("we_language", {}).get("score", 0)
        if we_score < 40:
            recommendations.append(
                {
                    "category": "BaÄŸ GÃ¼Ã§lendirme",
                    "title": "Biz-dili KullanÄ±n",
                    "description": "'Biz', 'bizim' gibi kelimeler kullanarak ortak hedeflerinizi vurgulayÄ±n.",
                }
            )

        # Denge
        balance_score = metrics.get("communication_balance", {}).get("score", 0)
        if balance_score < 50:
            recommendations.append(
                {
                    "category": "Ä°letiÅŸim",
                    "title": "Ä°letiÅŸim Dengesi",
                    "description": "Her iki taraf da eÅŸit ÅŸekilde kendini ifade etmeye Ã§alÄ±ÅŸsÄ±n.",
                }
            )

        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score < 60:
            recommendations.append(
                {
                    "category": "Empati",
                    "title": "Aktif Dinleme",
                    "description": "KarÅŸÄ±nÄ±zÄ± dinlerken 'AnladÄ±m', 'Seni anlÄ±yorum' gibi ifadeler kullanÄ±n.",
                }
            )

        return recommendations

    def generate_relationship_report(
        self,
        conversation_text: str,
        metrics: dict[str, Any],
        model_preference: str = "fast",
    ) -> dict[str, Any]:
        """
        Generate comprehensive relationship report with Gottman metrics (V2.0)

        Args:
            conversation_text: Full conversation text
            metrics: Basic metrics from analysis
            model_preference: 'fast' (GPT-4o-mini) or 'deep' (Claude-3.5-Sonnet)

        Returns:
            Structured relationship report (RelationshipReport schema)
        """
        start_time = time.time()

        # Cache key
        cache_key = self._get_cache_key("relationship_report_v2", metrics, conversation_text[:200])
        cached = cache_service.get(cache_key)

        if cached:
            logger.info(
                "Relationship report cache hit",
                extra={"latency_ms": (time.time() - start_time) * 1000},
            )
            return cached

        if not self._is_available():
            return self._fallback_relationship_report(metrics)

        try:
            # Context Management: Prepare conversation (summarize if too long)
            prepared_context = self._prepare_context(conversation_text, max_tokens=3000)
            
            # Build Gottman-based prompt with prepared context
            prompt = self._build_gottman_report_prompt(prepared_context, metrics)

            # Select model based on preference
            if model_preference == "deep" and self.provider == "anthropic":
                # Use Claude for deep analysis
                max_tokens = 2500
            else:
                # Use faster model
                max_tokens = 2000

            # Call LLM
            response = self._call_llm(prompt, max_tokens)

            # Parse structured JSON
            report = self._parse_relationship_report(response, metrics)

            # Cache
            cache_service.set(cache_key, report, ttl_seconds=7200)  # 2 hours

            logger.info(
                "Relationship report generated",
                extra={
                    "provider": self.provider,
                    "model_preference": model_preference,
                    "latency_ms": (time.time() - start_time) * 1000,
                    "gottman_score": report.get("genel_karne", {}).get("iliskki_sagligi", 0),
                },
            )

            return report

        except Exception as e:
            logger.error(
                "Relationship report generation failed",
                extra={"error": str(e), "provider": self.provider},
                exc_info=True,
            )
            return self._fallback_relationship_report(metrics)

    def _build_gottman_report_prompt(self, conversation_text: str, metrics: dict[str, Any]) -> str:
        """Build Gottman-based analysis prompt (Enforcing JSON Schema) - V3.0 Enhanced"""
        return f"""Sen bir Ä°liÅŸki PsikoloÄŸusun ve John Gottman'Ä±n 7 Prensibine gÃ¶re iliÅŸkileri analiz ediyorsun.

GÃ–REV: AÅŸaÄŸÄ±daki konuÅŸma metnini ve temel metrikleri kullanarak, Gottman Metodu Ã§erÃ§evesinde derinlemesine bir iliÅŸki analizi yap.

GÄ°RDÄ°LER:
1. KONUÅžMA METNÄ°:
{conversation_text[:3000]}... (devamÄ± var ama baÄŸlam iÃ§in bu kadarÄ± yeterli)

2. TEMEL METRÄ°KLER:
- Duygu Skoru: {metrics.get('sentiment', {}).get('score', 50)}
- Empati Skoru: {metrics.get('empathy', {}).get('score', 50)}
- Ã‡atÄ±ÅŸma Skoru: {metrics.get('conflict', {}).get('score', 50)}

Ã‡IKTI FORMATI (KESÄ°NLÄ°KLE JSON OLMALI):
AÅŸaÄŸÄ±daki JSON ÅŸemasÄ±na BÄ°REBÄ°R uyan bir yanÄ±t ver. Sadece JSON dÃ¶ndÃ¼r, markdown veya aÃ§Ä±klama ekleme.

{{
  "meta_data": {{
    "analiz_tarihi": "{datetime.now().isoformat()}",
    "model": "{self.provider}",
    "mesaj_sayisi": {len(conversation_text) // 50}, // Tahmini
    "platform": "generic"
  }},
  "genel_karne": {{
    "iliskki_sagligi": 0-100 arasÄ± bir puan,
    "overall_score": 0-10 arasÄ± bir puan (iliskki_sagligi / 10),
    "baskin_dinamik": "Ã–rn: Tutkulu ama Ã‡atÄ±ÅŸmalÄ±",
    "risk_seviyesi": "DÃ¼ÅŸÃ¼k" | "Orta" | "YÃ¼ksek" | "Kritik",
    "love_language_guess": "OnaylayÄ±cÄ± SÃ¶zler" | "Kaliteli Zaman" | "Hediye Almak" | "Hizmet Eylemleri" | "Fiziksel Temas" | null,
    "red_flags": ["UyarÄ± iÅŸareti 1", "UyarÄ± iÅŸareti 2"],
    "positive_traits": ["Pozitif Ã¶zellik 1", "Pozitif Ã¶zellik 2"]
  }},
  "gottman_bilesenleri": {{
    "sevgi_haritalari": {{ "skor": 0-100, "durum": "Ä°yi", "aciklama": "Partnerini tanÄ±ma dÃ¼zeyi..." }},
    "hayranlik_paylasimi": {{ "skor": 0-100, "durum": "Orta", "aciklama": "Takdir ve saygÄ±..." }},
    "yakinlasma_cabalari": {{ "skor": 0-100, "durum": "Kritik", "aciklama": "Ä°lgi gÃ¶sterme..." }},
    "olumlu_perspektif": {{ "skor": 0-100, "durum": "Ä°yi", "aciklama": "Genel bakÄ±ÅŸ..." }},
    "catisma_yonetimi": {{ "skor": 0-100, "durum": "GeliÅŸtirilmeli", "aciklama": "Kavga yÃ¶netimi..." }},
    "hayat_hayalleri": {{ "skor": 0-100, "durum": "Orta", "aciklama": "Ortak hedefler..." }},
    "ortak_anlam": {{ "skor": 0-100, "durum": "Ä°yi", "aciklama": "RitÃ¼eller ve anlam..." }}
  }},
  "duygusal_analiz": {{
    "iletisim_tonu": "Ã–rn: SavunmacÄ± / AÃ§Ä±k / Pasif-Agresif",
    "toksisite_seviyesi": 0-100,
    "yakinlik": 0-100,
    "duygu_ifadesi": "Ã–rn: Duygular bastÄ±rÄ±lÄ±yor / AÃ§Ä±kÃ§a ifade ediliyor",
    "empati_puani": 0-100
  }},
  "tespit_edilen_kaliplar": [
    {{
      "kalip": "Ã–rn: MahÅŸerin 4 AtlÄ±sÄ± - AÅŸaÄŸÄ±lama",
      "ornekler": ["Mesajdan alÄ±ntÄ± 1", "Mesajdan alÄ±ntÄ± 2"],
      "frekans": "DÃ¼ÅŸÃ¼k" | "Orta" | "YÃ¼ksek",
      "etki": "Pozitif" | "NÃ¶tr" | "Negatif"
    }}
  ],
  "aksiyon_onerileri": [
    {{
      "baslik": "Ã–rn: Mola Verin",
      "ornek_cumle": "Åžu an Ã§ok gerginim, 20 dakika sonra konuÅŸalÄ±m mÄ±?",
      "oncelik": "YÃ¼ksek" | "Orta" | "DÃ¼ÅŸÃ¼k",
      "kategori": "Ã‡atÄ±ÅŸma YÃ¶netimi"
    }}
  ],
  "ozel_notlar": ["GÃ¶zlem 1", "GÃ¶zlem 2"],
  "chart_data": {{
    "weekly_emotions": [
      {{ "label": "Pazartesi", "value": 65.5, "category": "positive" }},
      {{ "label": "SalÄ±", "value": 72.0, "category": "positive" }}
    ],
    "gottman_radar": [
      {{ "label": "Sevgi HaritalarÄ±", "value": 75 }},
      {{ "label": "HayranlÄ±k", "value": 68 }}
    ]
  }}
}}

Ã–NEMLÄ° NOTLAR:
1. love_language_guess: KonuÅŸmadan Ã§Ä±karÄ±m yaparak en uygun sevgi dilini tahmin et
2. red_flags: Dikkat edilmesi gereken uyarÄ± iÅŸaretlerini listele (max 5)
3. positive_traits: Ä°liÅŸkinin gÃ¼Ã§lÃ¼ yÃ¶nlerini listele (max 5)
4. chart_data: GÃ¶rselleÅŸtirme iÃ§in veri saÄŸla (weekly_emotions simÃ¼le edilebilir)
"""

    def _parse_relationship_report(self, response: str, metrics: dict[str, Any]) -> dict[str, Any]:
        """Parse relationship report using Pydantic validation"""
        from app.schemas.analysis import RelationshipReport

        try:
            # Clean response (remove markdown if present)
            cleaned_response = response.strip()
            if cleaned_response.startswith("```json"):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith("```"):
                cleaned_response = cleaned_response[:-3]

            cleaned_response = cleaned_response.strip()

            # Parse JSON
            # First try direct parsing
            report_data = json.loads(cleaned_response)

            # Validate with Pydantic
            validated_report = RelationshipReport(**report_data)

            return validated_report.dict()

        except json.JSONDecodeError as e:
            logger.error(f"JSON Decode Error in Relationship Report: {e}")
            logger.debug(f"Raw Response: {response[:500]}...")
            return self._fallback_relationship_report(metrics)
        except Exception as e:
            logger.error(f"Validation Error in Relationship Report: {e}")
            return self._fallback_relationship_report(metrics)

    def _parse_relationship_report(self, response: str, metrics: dict[str, Any]) -> dict[str, Any]:
        """Parse AI response into structured report"""
        try:
            # Extract JSON
            start = response.find("{")
            end = response.rfind("}") + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                report_data = json.loads(json_str)

                # Add metadata
                report_data["meta_data"] = {
                    "analiz_tarihi": time.strftime("%Y-%m-%dT%H:%M:%S"),
                    "model": self.provider,
                    "mesaj_sayisi": metrics.get("total_messages", 0),
                    "platform": metrics.get("platform", "unknown"),
                }

                return report_data

        except Exception as e:
            logger.error(f"Report parsing failed: {e}")

        return self._fallback_relationship_report(metrics)

    def _fallback_relationship_report(self, metrics: dict[str, Any]) -> dict[str, Any]:
        """Fallback report when AI is unavailable"""
        sentiment_score = metrics.get("sentiment", {}).get("score", 50)
        empathy_score = metrics.get("empathy", {}).get("score", 50)
        conflict_score = metrics.get("conflict", {}).get("score", 50)

        # Calculate overall health
        overall_health = int((sentiment_score + empathy_score + (100 - conflict_score)) / 3)

        return {
            "meta_data": {
                "analiz_tarihi": time.strftime("%Y-%m-%dT%H:%M:%S"),
                "model": "fallback",
                "mesaj_sayisi": metrics.get("total_messages", 0),
                "platform": "unknown",
            },
            "genel_karne": {
                "iliskki_sagligi": overall_health,
                "baskin_dinamik": "Standart Analiz (AI KullanÄ±lamÄ±yor)",
                "risk_seviyesi": "Orta" if overall_health < 50 else "DÃ¼ÅŸÃ¼k",
            },
            "gottman_bilesenleri": {
                "sevgi_haritalari": {"skor": 50, "durum": "Orta", "aciklama": "AI analizi gerekli"},
                "hayranlik_paylasimi": {
                    "skor": empathy_score,
                    "durum": "Orta",
                    "aciklama": "Empati skoruna dayalÄ±",
                },
                "yakinlasma_cabalari": {
                    "skor": 50,
                    "durum": "Orta",
                    "aciklama": "AI analizi gerekli",
                },
                "olumlu_perspektif": {
                    "skor": sentiment_score,
                    "durum": "Orta",
                    "aciklama": "Duygu skoruna dayalÄ±",
                },
                "catisma_yonetimi": {
                    "skor": 100 - conflict_score,
                    "durum": "Orta",
                    "aciklama": "Ã‡atÄ±ÅŸma skoruna dayalÄ±",
                },
                "hayat_hayalleri": {"skor": 50, "durum": "Orta", "aciklama": "AI analizi gerekli"},
                "ortak_anlam": {"skor": 50, "durum": "Orta", "aciklama": "AI analizi gerekli"},
            },
            "duygusal_analiz": {
                "iletisim_tonu": "NÃ¶tr",
                "toksisite_seviyesi": conflict_score,
                "yakinlik": empathy_score,
                "duygu_ifadesi": "KarÄ±ÅŸÄ±k",
            },
            "tespit_edilen_kaliplar": [
                {
                    "kalip": "Standart Ä°letiÅŸim",
                    "ornekler": ["AI analizi iÃ§in API key gerekli"],
                    "frekans": "Orta",
                    "etki": "NÃ¶tr",
                }
            ],
            "aksiyon_onerileri": [
                {
                    "baslik": "AI Analizi AktifleÅŸtirin",
                    "ornek_cumle": "Daha detaylÄ± analiz iÃ§in AI API key'i ekleyin",
                    "oncelik": "YÃ¼ksek",
                    "kategori": "Sistem",
                }
            ],
            "ozel_notlar": ["AI servisi kullanÄ±lamÄ±yor, temel metrikler gÃ¶steriliyor"],
        }

    def _build_insights_prompt_v3(self, metrics: dict[str, Any], summary: str) -> str:
        """Ä°Ã§gÃ¶rÃ¼ promptu oluÅŸtur (V3.0 - Strict JSON Schema)"""

        # Context optimization
        top_metrics = {
            "sentiment": metrics.get("sentiment", {}),
            "empathy": metrics.get("empathy", {}),
            "conflict": metrics.get("conflict", {}),
            "we_language": metrics.get("we_language", {}),
        }

        # RAG: Get relevant knowledge
        knowledge = get_relevant_knowledge(metrics)
        knowledge_context = format_knowledge_context(knowledge)

        return f"""Sen bir iliÅŸki psikoloÄŸusun. AÅŸaÄŸÄ±daki konuÅŸma analizine gÃ¶re derinlemesine iÃ§gÃ¶rÃ¼ler Ã¼ret.

{knowledge_context}

METRÄ°KLER:
{json.dumps(top_metrics, indent=2, ensure_ascii=False)}

KONUÅžMA Ã–ZETÄ°:
{summary}

GÃ–REV:
YukarÄ±daki metriklere ve psikoloji bilgilerine dayanarak 4-6 adet iÃ§gÃ¶rÃ¼ Ã¼ret.

Ä°Ã‡GÃ–RÃœ KATEGORÄ°LERÄ°:
- "GÃ¼Ã§lÃ¼ YÃ¶n": Ä°liÅŸkinin pozitif yÃ¶nleri
- "GeliÅŸim AlanÄ±": Ä°yileÅŸtirilebilecek noktalar
- "Dikkat NoktasÄ±": Dikkat edilmesi gereken riskler

KURALLAR:
- Her iÃ§gÃ¶rÃ¼ iÃ§in category, title, description, icon alanlarÄ± zorunlu
- title: Max 50 karakter, Ã¶z ve net
- description: 50-200 karakter arasÄ±, empatik ve destekleyici ton
- icon: Ä°lgili emoji (Ã¶rn: âœ…, ðŸ’¡, âš ï¸, ðŸ’)

Ã‡IKTI FORMATI:
{{
  "insights": [
    {{
      "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
      "title": "YÃ¼ksek Empati Seviyesi",
      "description": "Ä°letiÅŸimde karÅŸÄ±nÄ±zÄ± anlamaya yÃ¶nelik gÃ¼Ã§lÃ¼ Ã§aba var. Bu, iliÅŸkide gÃ¼ven ve yakÄ±nlÄ±k oluÅŸturmanÄ±n temel taÅŸÄ±dÄ±r.",
      "icon": "ðŸ’"
    }},
    {{
      "category": "GeliÅŸim AlanÄ±",
      "title": "Biz-dili KullanÄ±mÄ±",
      "description": "Bireysel ifadeler aÄŸÄ±rlÄ±kta. 'Biz', 'birlikte' gibi kelimeler kullanarak ortaklÄ±k hissini gÃ¼Ã§lendirebilirsiniz.",
      "icon": "ðŸ’¡"
    }}
  ]
}}"""

    # ==================== Context Management (Stage 1) ====================

    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count (rough approximation: 1 token â‰ˆ 4 chars)"""
        return len(text) // 4

    def _should_summarize(self, conversation_text: str, max_tokens: int = 3000) -> bool:
        """Check if conversation needs summarization"""
        estimated_tokens = self._estimate_tokens(conversation_text)
        return estimated_tokens > max_tokens

    def _summarize_conversation(self, conversation_text: str, max_summary_tokens: int = 1000) -> str:
        """Summarize long conversations to fit within token limits
        
        Args:
            conversation_text: Full conversation text
            max_summary_tokens: Maximum tokens for summary
            
        Returns:
            Summarized conversation text
        """
        if not self._is_available():
            # Fallback: truncate to first and last parts
            max_chars = max_summary_tokens * 4
            if len(conversation_text) <= max_chars:
                return conversation_text
            
            half = max_chars // 2
            return f"{conversation_text[:half]}\n\n[...Ã¶zetlendi...]}\n\n{conversation_text[-half:]}"

        try:
            prompt = f"""AÅŸaÄŸÄ±daki iliÅŸki konuÅŸmasÄ±nÄ± Ã¶zetle. Ã–nemli duygusal tonlarÄ±, Ã§atÄ±ÅŸma noktalarÄ±nÄ± ve olumlu anlarÄ± koru.

KONUÅžMA:
{conversation_text[:12000]}  # Limit input to ~3000 tokens

KISA Ã–ZET (max 500 kelime):"""

            summary = self._call_llm(
                prompt=prompt,
                max_tokens=max_summary_tokens,
                temperature=0.3  # Lower temperature for factual summarization
            )

            logger.info(
                "Conversation summarized",
                extra={
                    "original_tokens": self._estimate_tokens(conversation_text),
                    "summary_tokens": self._estimate_tokens(summary),
                    "compression_ratio": round(len(summary) / len(conversation_text), 2)
                }
            )

            return summary

        except Exception as e:
            logger.error(f"Summarization failed: {e}")
            # Fallback to truncation
            max_chars = max_summary_tokens * 4
            return conversation_text[:max_chars]

    def _prepare_context(self, conversation_text: str, max_tokens: int = 3000) -> str:
        """Prepare conversation context with automatic summarization if needed
        
        Args:
            conversation_text: Full conversation text
            max_tokens: Maximum tokens allowed
            
        Returns:
            Prepared context (original or summarized)
        """
        if self._should_summarize(conversation_text, max_tokens):
            logger.info("Conversation exceeds token limit, summarizing...")
            return self._summarize_conversation(conversation_text, max_summary_tokens=max_tokens)
        
        return conversation_text

    # ==================== Stage 3: Module Functionality ====================

    def tone_shift(
        self,
        message: str,
        target_tone: str = "polite",
        max_tokens: int = 200
    ) -> dict[str, Any]:
        """Rewrite message in a different tone (Tone Shifter)
        
        Args:
            message: Original message
            target_tone: Target tone (polite, empathetic, assertive, calm)
            max_tokens: Max tokens for response
            
        Returns:
            Dict with rewritten message and explanation
        """
        if not self._is_available():
            return {
                "original": message,
                "rewritten": message,
                "tone": target_tone,
                "explanation": "AI servisi kullanÄ±lamÄ±yor"
            }

        tone_instructions = {
            "polite": "Kibarca ve saygÄ±lÄ± bir ÅŸekilde, 'Ben dili' kullanarak",
            "empathetic": "Empatik ve anlayÄ±ÅŸlÄ± bir ÅŸekilde, karÅŸÄ± tarafÄ±n duygularÄ±nÄ± dikkate alarak",
            "assertive": "Net ve kendinden emin ama saygÄ±lÄ± bir ÅŸekilde",
            "calm": "Sakin ve duygusal olmayan, yapÄ±cÄ± bir ÅŸekilde"
        }

        instruction = tone_instructions.get(target_tone, tone_instructions["polite"])

        prompt = f"""AÅŸaÄŸÄ±daki mesajÄ± {instruction} yeniden yaz.

ORIJINAL MESAJ:
"{message}"

KURALLAR:
1. 'Ben dili' kullan (Ben hissediyorum, Ben dÃ¼ÅŸÃ¼nÃ¼yorum)
2. SuÃ§layÄ±cÄ± ifadelerden kaÃ§Ä±n
3. Somut ve yapÄ±cÄ± ol
4. KÄ±sa ve Ã¶z tut (max 2-3 cÃ¼mle)

YENÄ°DEN YAZILMIÅž MESAJ:"""

        try:
            rewritten = self._call_llm(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=0.7
            )

            # Extract just the message (remove any extra explanation)
            rewritten = rewritten.strip().strip('"').strip()

            return {
                "original": message,
                "rewritten": rewritten,
                "tone": target_tone,
                "explanation": f"{target_tone.capitalize()} tonda yeniden yazÄ±ldÄ±"
            }

        except Exception as e:
            logger.error(f"Tone shift failed: {e}")
            return {
                "original": message,
                "rewritten": message,
                "tone": target_tone,
                "explanation": f"Hata: {str(e)}"
            }

    def suggest_conflict_action(
        self,
        conversation_text: str,
        max_tokens: int = 300
    ) -> dict[str, Any]:
        """Suggest immediate action for conflict resolution
        
        Args:
            conversation_text: Recent conversation showing conflict
            max_tokens: Max tokens for response
            
        Returns:
            Dict with action suggestion
        """
        if not self._is_available():
            return {
                "action": "Mola verin",
                "reason": "Gerginlik yÃ¼ksek gÃ¶rÃ¼nÃ¼yor",
                "how": "20 dakika ara verin ve sakinleÅŸin",
                "priority": "high"
            }

        prompt = f"""AÅŸaÄŸÄ±daki Ã§atÄ±ÅŸmalÄ± konuÅŸmaya bakarak ANLÄ°K bir aksiyon Ã¶nerisi sun.

KONUÅžMA:
{conversation_text[:1000]}

GÃ–REV: Tek bir somut aksiyon Ã¶ner. JSON formatÄ±nda dÃ¶ndÃ¼r:

{{
  "action": "Ã–rn: Mola Verin",
  "reason": "Neden bu aksiyonu Ã¶neriyorsun (1 cÃ¼mle)",
  "how": "NasÄ±l yapÄ±lacaÄŸÄ± (somut adÄ±mlar, 1-2 cÃ¼mle)",
  "priority": "high|medium|low"
}}

Sadece JSON dÃ¶ndÃ¼r, baÅŸka aÃ§Ä±klama ekleme."""

        try:
            response = self._call_llm(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=0.5
            )

            # Parse JSON
            import json
            # Clean response
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()

            suggestion = json.loads(cleaned)

            return {
                "action": suggestion.get("action", "Mola verin"),
                "reason": suggestion.get("reason", ""),
                "how": suggestion.get("how", ""),
                "priority": suggestion.get("priority", "medium")
            }

        except Exception as e:
            logger.error(f"Conflict action suggestion failed: {e}")
            return {
                "action": "Mola Verin",
                "reason": "GerginliÄŸi azaltmak iÃ§in",
                "how": "20 dakika ara verin, sakinleÅŸin ve sonra konuÅŸmaya devam edin",
                "priority": "high"
            }



# Singleton instance
_ai_service_instance = None


def get_ai_service() -> AIService:
    """AI service singleton"""
    global _ai_service_instance
    if _ai_service_instance is None:
        _ai_service_instance = AIService()
    return _ai_service_instance
