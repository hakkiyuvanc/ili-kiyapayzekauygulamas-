"""AI Service - LLM Entegrasyonu"""

import os
import json
import logging
import hashlib
import time
from typing import Dict, Any, Optional, List
from datetime import datetime
from openai import OpenAI
from anthropic import Anthropic

import google.generativeai as genai
from app.core.config import settings
from app.services.cache_service import cache_service
from app.services.knowledge_base import get_relevant_knowledge, format_knowledge_context

logger = logging.getLogger(__name__)

class AIService:
    """Yapay zeka servisi - OpenAI/Anthropic/Gemini entegrasyonu"""
    
    PROMPT_VERSION = "v2.1"  # Prompt versioning
    
    def __init__(self):
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        self.provider = settings.AI_PROVIDER
        
        # API anahtarlarÄ±nÄ± kontrol et
        if self.provider == "openai":
            api_key = settings.OPENAI_API_KEY
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
        elif self.provider == "anthropic":
            api_key = settings.ANTHROPIC_API_KEY
            if api_key:
                self.anthropic_client = Anthropic(api_key=api_key)
        elif self.provider == "gemini":
            api_key = settings.GEMINI_API_KEY
            if api_key:
                genai.configure(api_key=api_key)
                self.gemini_client = genai.GenerativeModel(settings.GEMINI_MODEL)
        
        # Structured logging
        if self._is_available():
            logger.info("AI Service initialized", extra={
                "provider": self.provider,
                "prompt_version": self.PROMPT_VERSION,
                "status": "available"
            })
        else:
            logger.warning("AI Service initialized without provider", extra={
                "provider": self.provider,
                "status": "fallback_mode"
            })

    def generate_insights(
        self,
        metrics: Dict[str, Any],
        conversation_summary: str,
        max_tokens: int = 1000
    ) -> List[Dict[str, str]]:
        """
        AI ile derinlemesine iÃ§gÃ¶rÃ¼ler oluÅŸtur (with caching & monitoring)
        
        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            Ä°Ã§gÃ¶rÃ¼ listesi
        """
        start_time = time.time()
        
        # Check cache first
        cache_key = self._get_cache_key("insights", metrics, conversation_summary)
        cached = cache_service.get(cache_key)
        
        if cached:
            logger.info("AI insights cache hit", extra={
                "cache_key": cache_key[:20],
                "latency_ms": (time.time() - start_time) * 1000
            })
            return cached
        
        if not self._is_available():
            fallback = self._fallback_insights(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback
        
        try:
            # First attempt with enhanced prompt
            prompt = self._build_insights_prompt(metrics, conversation_summary)
            response = self._call_llm(prompt, max_tokens)
            insights = self._parse_insights_response(response)
            
            if insights:
                # Cache successful response
                cache_service.set(cache_key, insights, ttl_seconds=3600)  # 1 hour
                
                # Log success with metrics
                logger.info("AI insights generated successfully", extra={
                    "provider": self.provider,
                    "insights_count": len(insights),
                    "latency_ms": (time.time() - start_time) * 1000,
                    "cache_miss": True,
                    "prompt_version": self.PROMPT_VERSION
                })
                return insights
            
            # Retry with simplified prompt on parsing failure
            logger.warning("Retrying with simplified prompt", extra={"reason": "parsing_failure"})
            simple_prompt = self._build_simple_insights_prompt(metrics)
            response = self._call_llm(simple_prompt, max_tokens)
            insights = self._parse_insights_response(response)
            
            if insights:
                cache_service.set(cache_key, insights, ttl_seconds=1800)  # 30 min (lower quality)
            
            result = insights if insights else self._fallback_insights(metrics)
            cache_service.set(cache_key, result, ttl_seconds=3600)
            return result
            
        except Exception as e:
            logger.error("AI insights generation failed", extra={
                "error": str(e),
                "provider": self.provider,
                "latency_ms": (time.time() - start_time) * 1000
            }, exc_info=True)
            
            fallback = self._fallback_insights(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

    def generate_recommendations(
        self,
        metrics: Dict[str, Any],
        insights: List[Dict[str, str]],
        max_tokens: int = 800
    ) -> List[Dict[str, str]]:
        """
        AI ile kiÅŸiselleÅŸtirilmiÅŸ Ã¶neriler oluÅŸtur (with caching)
        """
        start_time = time.time()
        
        # Check cache
        cache_key = self._get_cache_key("recommendations", metrics, str(insights))
        cached = cache_service.get(cache_key)
        
        if cached:
            logger.info("AI recommendations cache hit", extra={
                "latency_ms": (time.time() - start_time) * 1000
            })
            return cached
        
        if not self._is_available():
            fallback = self._fallback_recommendations(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback
        
        try:
            prompt = self._build_recommendations_prompt(metrics, insights)
            response = self._call_llm(prompt, max_tokens)
            recommendations = self._parse_recommendations_response(response)
            
            # Cache and log
            cache_service.set(cache_key, recommendations, ttl_seconds=3600)
            
            logger.info("AI recommendations generated", extra={
                "provider": self.provider,
                "recommendations_count": len(recommendations),
                "latency_ms": (time.time() - start_time) * 1000,
                "prompt_version": self.PROMPT_VERSION
            })
            
            return recommendations
            
        except Exception as e:
            logger.error("AI recommendations generation failed", extra={
                "error": str(e),
                "provider": self.provider,
                "latency_ms": (time.time() - start_time) * 1000
            }, exc_info=True)
            
            fallback = self._fallback_recommendations(metrics)
            cache_service.set(cache_key, fallback, ttl_seconds=3600)
            return fallback

    def chat_with_coach(
        self,
        message: str,
        history: List[Dict[str, str]],
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        AI Ä°liÅŸki KoÃ§u ile sohbet et
        """
        if not self._is_available():
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda AI servislerine eriÅŸemiyorum. LÃ¼tfen daha sonra tekrar deneyin."

        system_prompt = """Sen profesyonel, empatik ve Ã§Ã¶zÃ¼m odaklÄ± bir Ä°liÅŸki KoÃ§usun. 
        KullanÄ±cÄ±larÄ±n iliÅŸki sorunlarÄ±nÄ± dinler, yargÄ±lamadan analiz eder ve yapÄ±cÄ± tavsiyeler verirsin.
        EÄŸer bir analiz raporu baÄŸlamÄ± varsa, cevaplarÄ±nÄ± bu rapora dayandÄ±r.
        KÄ±sa, net ve samimi cevaplar ver. Emoji kullanabilirsin."""

        if context:
            system_prompt += f"\n\nBAÄžLAM (Analiz Raporu):\n{json.dumps(context, ensure_ascii=False)}"

        messages = [{"role": "system", "content": system_prompt}]
        
        # History format: [{'role': 'user', 'content': '...'}, ...]
        # Limit history to last 10 messages to save tokens
        for msg in history[-10:]:
            messages.append({"role": msg["role"], "content": msg["content"]})
            
        messages.append({"role": "user", "content": message})

        try:
            if self.provider == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",  # or gpt-3.5-turbo
                    messages=messages,
                    max_tokens=500,
                    temperature=0.7
                )
                return response.choices[0].message.content
                
            elif self.provider == "gemini" and self.gemini_client:
                # Convert history to Gemini format
                gemini_history = []
                for msg in history[-10:]:
                    role = "user" if msg["role"] == "user" else "model"
                    gemini_history.append({"role": role, "parts": [msg["content"]]})
                
                # Add system prompt as the first message or configure it in model?
                # Gemini doesn't strictly have system message in chat history the same way.
                # We can prepend it to the first user message or use system_instruction if available in newer lib.
                # For compatibility, let's prepend system prompt context to the current message or start of history.
                
                # Simple approach: Prepend system prompt to the last message call for this turn
                full_message = f"{system_prompt}\n\nUSER MESSAGE: {message}"
                
                chat = self.gemini_client.start_chat(history=gemini_history)
                response = chat.send_message(full_message)
                return response.text
                
            # Default fallback
            return "AI saÄŸlayÄ±cÄ± yapÄ±landÄ±rmasÄ± eksik."
            
        except Exception as e:
            logger.error("AI chat failed", extra={
                "error": str(e),
                "provider": self.provider,
                "message_length": len(message)
            }, exc_info=True)
            return "ÃœzgÃ¼nÃ¼m, ÅŸu anda bir sorun yaÅŸÄ±yorum. LÃ¼tfen daha sonra tekrar deneyin."

    def enhance_summary(
        self,
        basic_summary: str,
        metrics: Dict[str, Any],
        max_tokens: int = 500
    ) -> str:
        """
        AI ile Ã¶zeti geliÅŸtir
        
        Args:
            basic_summary: Temel Ã¶zet
            metrics: Metrikler
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            GeliÅŸtirilmiÅŸ Ã¶zet
        """
        if not self._is_available():
            return basic_summary

        prompt = self._build_summary_prompt(basic_summary, metrics)
        
        try:
            return self._call_llm(prompt, max_tokens)
        except Exception:
            return basic_summary

    def _build_insights_prompt(self, metrics: Dict[str, Any], summary: str) -> str:
        """Ä°Ã§gÃ¶rÃ¼ promptu oluÅŸtur (improved with few-shot & chain-of-thought & knowledge)"""
        
        # Context optimization: Extract top metrics only
        top_metrics = {
            "sentiment": metrics.get("sentiment", {}),
            "empathy": metrics.get("empathy", {}),
            "conflict": metrics.get("conflict", {}),
            "we_language": metrics.get("we_language", {})
        }
        
        # RAG Quick Win: Get relevant knowledge snippets
        knowledge = get_relevant_knowledge(metrics)
        knowledge_context = format_knowledge_context(knowledge)
        
        return f"""
{knowledge_context}
Sen bir iliÅŸki psikoloÄŸusun. AÅŸaÄŸÄ±daki gÃ¶revi adÄ±m adÄ±m yap:

1. ADIM: Metrikleri incele
{json.dumps(top_metrics, indent=2, ensure_ascii=False)}

2. ADIM: KonuÅŸma Ã¶zetini oku
{summary}

3. ADIM: YukarÄ±daki psikoloji bilgilerini referans alarak metriklere gÃ¶re iÃ§gÃ¶rÃ¼ler Ã¼ret

Ã–RNEK Ã‡IKTILAR (Referans iÃ§in):
[
  {{
    "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
    "title": "YÃ¼ksek Empati Seviyesi",
    "description": "Ä°letiÅŸimde karÅŸÄ±nÄ±zÄ± anlamaya yÃ¶nelik gÃ¼Ã§lÃ¼ Ã§aba var. Bu, iliÅŸkide gÃ¼ven ve yakÄ±nlÄ±k oluÅŸturmanÄ±n temel taÅŸÄ±dÄ±r."
  }},
  {{
    "category": "GeliÅŸim AlanÄ±",
    "title": "Biz-dili KullanÄ±mÄ± ZayÄ±f",
    "description": "Bireysel ifadeler aÄŸÄ±rlÄ±kta. 'Biz', 'birlikte' gibi kelimeler kullanarak ortaklÄ±k hissini gÃ¼Ã§lendirebilirsiniz."
  }},
  {{
    "category": "Dikkat NoktasÄ±",
    "title": "Ã‡atÄ±ÅŸma YÃ¶netimi",
    "description": "AnlaÅŸmazlÄ±klarda savunma moduna geÃ§me eÄŸilimi var. AÃ§Ä±k ve sakin iletiÅŸim Ã¶nemli."
  }}
]

4. ADIM: YukarÄ±daki metriklere ve psikoloji bilgilerine gÃ¶re 4-6 adet benzeri iÃ§gÃ¶rÃ¼ Ã¼ret

FORMAT KURALLARI:
- category: sadece "GÃ¼Ã§lÃ¼ YÃ¶n", "GeliÅŸim AlanÄ±", veya "Dikkat NoktasÄ±"
- title: max 50 karakter, TÃ¼rkÃ§e
- description: 100-150 karakter, empatik ve destekleyici ton

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver (Markdown yok):
[
  {"category": "GÃ¼Ã§lÃ¼ YÃ¶n", "title": "...", "description": "..."},
  {"category": "GeliÅŸim AlanÄ±", "title": "...", "description": "..."}
]"""

    def _build_recommendations_prompt(self, metrics: Dict[str, Any], insights: List[Dict]) -> str:
        """Ã–neri promptu oluÅŸtur (improved with few-shot)"""
        
        # Context optimization: Top insights only
        top_insights = insights[:4] if len(insights) > 4 else insights
        
        return f"""
Sen bir iliÅŸki koÃ§usun. AÅŸaÄŸÄ±daki gÃ¶revi adÄ±m adÄ±m yap:

1. ADIM: Ä°Ã§gÃ¶rÃ¼leri oku
{json.dumps(top_insights, indent=2, ensure_ascii=False)}

2. ADIM: Metriklerdeki zayÄ±f alanlarÄ± tespit et
- Empati skoru: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma skoru: {metrics.get('conflict', {}).get('score', 'N/A')}
- Biz-dili skoru: {metrics.get('we_language', {}).get('score', 'N/A')}

3. ADIM: Somut, uygulanabilir Ã¶neriler oluÅŸtur

Ã–RNEK Ã‡IKTILAR (Referans iÃ§in):
[
  {{
    "category": "BaÄŸ GÃ¼Ã§lendirme",
    "title": "Ortak Hedefler Belirleyin",
    "description": "'Bizim iÃ§in ne iyi?' sorusunu sorun. HaftalÄ±k bir 'biz' planÄ± yapÄ±n ve birlikte karar alÄ±n."
  }},
  {{
    "category": "Ä°letiÅŸim",
    "title": "GÃ¼nlÃ¼k Check-in Rutini",
    "description": "Her gÃ¼n 10 dakika telefonlar kapalÄ± konuÅŸun. Sadece dinleyin ve 'AnlÄ±yorum' deyin."
  }},
  {{
    "category": "Empati",
    "title": "Duygu YansÄ±tma PratiÄŸi",
    "description": "KarÅŸÄ±nÄ±zÄ±n sÃ¶ylediklerini kendi cÃ¼mlelerinizle tekrarlayÄ±n. 'Senin iÃ§in bu zor olmalÄ±' gibi."
  }}
]

4. ADIM: YukarÄ±daki iÃ§gÃ¶rÃ¼lere gÃ¶re 4-5 adet benzeri Ã¶neri Ã¼ret

FORMAT KURALLARI:
- category: "Ä°letiÅŸim", "Empati", "Ã‡atÄ±ÅŸma YÃ¶netimi", veya "BaÄŸ GÃ¼Ã§lendirme"
- title: max 50 karakter, eyleme yÃ¶nelik
- description: 100-150 karakter, somut adÄ±mlar iÃ§ermeli

Ã‡Ä±ktÄ±nÄ± JSON array formatÄ±nda ver:
"""

    def _build_summary_prompt(self, basic_summary: str, metrics: Dict[str, Any]) -> str:
        """Ã–zet geliÅŸtirme promptu"""
        return f"""
AÅŸaÄŸÄ±daki iliÅŸki analizi Ã¶zetini daha anlaÅŸÄ±lÄ±r ve empatik hale getir:

MEVCUT Ã–ZET:
{basic_summary}

METRIKLER:
- Duygu Skoru: {metrics.get('sentiment', {}).get('score', 'N/A')}
- Empati Skoru: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma Skoru: {metrics.get('conflict', {}).get('score', 'N/A')}

KÄ±sa (2-3 cÃ¼mle), destekleyici ve yapÄ±cÄ± bir Ã¶zet oluÅŸtur. TÃ¼rkÃ§e kullan.
"""

    def _build_simple_insights_prompt(self, metrics: Dict[str, Any]) -> str:
        """Simplified prompt for retry (error recovery)"""
        return f"""
Ä°letiÅŸim analizi iÃ§in iÃ§gÃ¶rÃ¼ler Ã¼ret:

Metrikler:
- Duygu: {metrics.get('sentiment', {}).get('score', 'N/A')}
- Empati: {metrics.get('empathy', {}).get('score', 'N/A')}
- Ã‡atÄ±ÅŸma: {metrics.get('conflict', {}).get('score', 'N/A')}

3-4 kÄ±sa iÃ§gÃ¶rÃ¼ JSON formatÄ±nda ver:
[{{"category": "GÃ¼Ã§lÃ¼ YÃ¶n", "title": "...", "description": "..."}}]
"""

    def _call_llm(self, prompt: str, max_tokens: int) -> str:
        """LLM Ã§aÄŸrÄ±sÄ± yap"""
        if self.provider == "openai" and self.openai_client:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": "Sen TÃ¼rkÃ§e konuÅŸan profesyonel bir iliÅŸki terapistisin."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        elif self.provider == "anthropic" and self.anthropic_client:
            response = self.anthropic_client.messages.create(
                model=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet-20241022"),
                max_tokens=max_tokens,
                temperature=0.7,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            return response.content[0].text.strip()
            
        elif self.provider == "gemini" and self.gemini_client:
            response = self.gemini_client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=max_tokens,
                    temperature=0.7,
                )
            )
            return response.text.strip()
        
        raise Exception("AI provider yapÄ±landÄ±rÄ±lmamÄ±ÅŸ")

    def _parse_insights_response(self, response: str) -> List[Dict[str, str]]:
        """AI yanÄ±tÄ±ndan iÃ§gÃ¶rÃ¼leri parse et"""
        try:
            # JSON array'i bul ve parse et
            start = response.find('[')
            end = response.rfind(']') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                insights = json.loads(json_str)
                return insights[:6]  # Maksimum 6 iÃ§gÃ¶rÃ¼
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")
        
        return []

    def _parse_recommendations_response(self, response: str) -> List[Dict[str, str]]:
        """AI yanÄ±tÄ±ndan Ã¶nerileri parse et"""
        try:
            start = response.find('[')
            end = response.rfind(']') + 1
            if start != -1 and end > start:
                json_str = response[start:end]
                recommendations = json.loads(json_str)
                return recommendations[:5]  # Maksimum 5 Ã¶neri
        except Exception as e:
            print(f"Parse hatasÄ±: {e}")
        
        return []

    
    def generate_reply_suggestions(
        self,
        metrics: Dict[str, Any],
        conversation_summary: str,
        max_tokens: int = 500
    ) -> List[str]:
        """
        AI ile cevap Ã¶nerileri oluÅŸtur
        
        Args:
            metrics: HesaplanmÄ±ÅŸ metrikler
            conversation_summary: KonuÅŸma Ã¶zeti
            max_tokens: Maksimum token sayÄ±sÄ±
            
        Returns:
            Cevap Ã¶nerileri listesi
        """
        if not self._is_available():
            return self._fallback_reply_suggestions()

        prompt = self._build_reply_suggestions_prompt(metrics, conversation_summary)
        
        try:
            response = self._call_llm(prompt, max_tokens)
            suggestions = self._parse_reply_suggestions_response(response)
            return suggestions
        except Exception as e:
            logger.error("AI reply suggestions failed", extra={
                "error": str(e),
                "provider": self.provider
            }, exc_info=True)
            return self._fallback_reply_suggestions()

    def _fallback_reply_suggestions(self) -> List[str]:
        """AI kullanÄ±lamadÄ±ÄŸÄ±nda varsayÄ±lan cevap Ã¶nerileri"""
        return [
            "AnlÄ±yorum, bu konuya farklÄ± bir aÃ§Ä±dan bakabiliriz.",
            "DuygularÄ±nÄ± paylaÅŸtÄ±ÄŸÄ±n iÃ§in teÅŸekkÃ¼r ederim, seni daha iyi anlamak istiyorum.",
            "Bu durum beni de dÃ¼ÅŸÃ¼ndÃ¼rÃ¼yor, ortak bir Ã§Ã¶zÃ¼m bulalÄ±m."
        ]

    def _build_reply_suggestions_prompt(self, metrics: Dict[str, Any], summary: str) -> str:
        """Cevap Ã¶nerileri iÃ§in prompt oluÅŸtur"""
        return f"""
        AÅŸaÄŸÄ±daki iliÅŸki analizine dayanarak, kullanÄ±cÄ±nÄ±n karÅŸÄ± tarafa yazabileceÄŸi 3 farklÄ± cevap seÃ§eneÄŸi Ã¶ner.
        
        Analiz Ã–zeti: {summary}
        Duygu Durumu: {metrics.get('sentiment', {}).get('score', 50)}/100
        Empati Seviyesi: {metrics.get('empathy', {}).get('score', 50)}/100
        
        LÃ¼tfen ÅŸu 3 farklÄ± tonda cevap Ã¶nerisi sun:
        1. YapÄ±cÄ± ve Ã‡Ã¶zÃ¼m OdaklÄ±
        2. Empatik ve Duygusal
        3. Net ve SÄ±nÄ±r Koyucu
        
        Format: Sadece 3 madde halinde cevap metinlerini yaz. BaÅŸka aÃ§Ä±klama ekleme.
        """

    def _parse_reply_suggestions_response(self, response: str) -> List[str]:
        """AI yanÄ±tÄ±ndan cevap Ã¶nerilerini ayÄ±kla"""
        try:
            # SatÄ±r satÄ±r ayÄ±r ve temizle
            lines = [line.strip() for line in response.strip().split('\n') if line.strip()]
            suggestions = []
            for line in lines:
                # NumaralandÄ±rmayÄ± temizle (1. , - vb.)
                cleaned = line.lstrip('1234567890.-*â€¢ ')
                if cleaned:
                    suggestions.append(cleaned)
            
            return suggestions[:3]  # En fazla 3 Ã¶neri
        except Exception:
            return self._fallback_reply_suggestions()

    
    def _get_cache_key(self, operation: str, metrics: Dict, context: str = "") -> str:
        """Generate cache key for AI responses"""
        data = {
            "operation": operation,
            "metrics": {k: v.get("score") if isinstance(v, dict) else v for k, v in metrics.items()},
            "context": context[:100] if context else "",  # First 100 chars only
            "version": self.PROMPT_VERSION
        }
        data_str = json.dumps(data, sort_keys=True)
        hash_key = hashlib.md5(data_str.encode()).hexdigest()
        return f"ai_{operation}:{hash_key}"
    
    def _is_available(self) -> bool:
        """AI servisi kullanÄ±labilir mi?"""
        return (self.openai_client is not None) or (self.anthropic_client is not None) or (self.gemini_client is not None)

    def _fallback_insights(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """AI yoksa fallback iÃ§gÃ¶rÃ¼ler"""
        insights = []
        
        # Duygu analizi
        sentiment_score = metrics.get("sentiment", {}).get("score", 0)
        if sentiment_score >= 60:
            insights.append({
                "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                "title": "Olumlu Ä°letiÅŸim",
                "description": "Ä°letiÅŸiminiz genel olarak pozitif ve destekleyici bir ton taÅŸÄ±yor.",
                "icon": "âœ…"
            })
        elif sentiment_score < 40:
            insights.append({
                "category": "GeliÅŸim AlanÄ±",
                "title": "Duygusal Ton",
                "description": "Ä°letiÅŸimde daha olumlu bir dil kullanmaya Ã¶zen gÃ¶sterin.",
                "icon": "ðŸ’¡"
            })
        
        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score >= 60:
            insights.append({
                "category": "GÃ¼Ã§lÃ¼ YÃ¶n",
                "title": "YÃ¼ksek Empati",
                "description": "KarÅŸÄ±nÄ±zdakinin duygularÄ±nÄ± anlamaya Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±z aÃ§Ä±kÃ§a gÃ¶rÃ¼lÃ¼yor.",
                "icon": "ðŸ’"
            })
        else:
            insights.append({
                "category": "GeliÅŸim AlanÄ±",
                "title": "Empati GeliÅŸtirme",
                "description": "Daha fazla empatik ifade kullanarak iletiÅŸimi gÃ¼Ã§lendirebilirsiniz.",
                "icon": "ðŸ’¡"
            })
        
        # Ã‡atÄ±ÅŸma
        conflict_score = metrics.get("conflict", {}).get("score", 0)
        if conflict_score > 50:
            insights.append({
                "category": "Dikkat NoktasÄ±",
                "title": "Ã‡atÄ±ÅŸma Belirtileri",
                "description": "KonuÅŸmada gerginlik iÅŸaretleri var. Sakin ve yapÄ±cÄ± iletiÅŸime odaklanÄ±n.",
                "icon": "âš ï¸"
            })
        
        return insights

    def _fallback_recommendations(self, metrics: Dict[str, Any]) -> List[Dict[str, str]]:
        """AI yoksa fallback Ã¶neriler"""
        recommendations = []
        
        # Biz-dili
        we_score = metrics.get("we_language", {}).get("score", 0)
        if we_score < 40:
            recommendations.append({
                "category": "BaÄŸ GÃ¼Ã§lendirme",
                "title": "Biz-dili KullanÄ±n",
                "description": "'Biz', 'bizim' gibi kelimeler kullanarak ortak hedeflerinizi vurgulayÄ±n."
            })
        
        # Denge
        balance_score = metrics.get("communication_balance", {}).get("score", 0)
        if balance_score < 50:
            recommendations.append({
                "category": "Ä°letiÅŸim",
                "title": "Ä°letiÅŸim Dengesi",
                "description": "Her iki taraf da eÅŸit ÅŸekilde kendini ifade etmeye Ã§alÄ±ÅŸsÄ±n."
            })
        
        # Empati
        empathy_score = metrics.get("empathy", {}).get("score", 0)
        if empathy_score < 60:
            recommendations.append({
                "category": "Empati",
                "title": "Aktif Dinleme",
                "description": "KarÅŸÄ±nÄ±zÄ± dinlerken 'AnladÄ±m', 'Seni anlÄ±yorum' gibi ifadeler kullanÄ±n."
            })
        
        return recommendations


# Singleton instance
_ai_service_instance = None


def get_ai_service() -> AIService:
    """AI service singleton"""
    global _ai_service_instance
    if _ai_service_instance is None:
        _ai_service_instance = AIService()
    return _ai_service_instance
